{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb7aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "\n",
    "import collections\n",
    "import re  #python中的正则模块\n",
    "import numpy\n",
    "from  numpy import *\n",
    "#from d2l import torch as d2l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cf0c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8.3 ###\n",
    "\n",
    "#使用随机抽样从数据中生成小批量子序列\n",
    "def seq_data_iter_random(corpus,batch_size,num_steps):\n",
    "    corpus=corpus[random.randint(0,num_steps-1):] #random.randint(0,num_step-1)表示选择任意偏移量作为初始位置\n",
    "    num_subseqs=(len(corpus)-1) // num_steps #减去1：考虑标签\n",
    "    initia_indices=list(range(0,num_subseqs*num_steps,num_steps))\n",
    "    random.shuffle(initia_indices) #小批量中的子序列不一定在原始序列上相邻\n",
    "    \n",
    "    def data(pos):\n",
    "        return corpus[pos:pos+num_steps] # 返回从`pos`位置开始的长度为`num_steps`的序列\n",
    "    \n",
    "    num_batches=num_subseqs // batch_size  #num_batches 批量数目\n",
    "    for i in range(0,batch_size*num_batches,batch_size):\n",
    "        initia_indices_per_batch=initia_indices[i:i+batch_size]\n",
    "        X=[data(j) for j in initia_indices_per_batch]\n",
    "        Y=[data(j+1) for j in initia_indices_per_batch]\n",
    "        yield torch.tensor(X),torch.tensor(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8.5 RNN模型-不使用框架###\n",
    "\n",
    "#RNN模型---不使用框架&使用one-hot encoding\n",
    "\n",
    "import math \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size,num_steps=32,35\n",
    "train_iter,vocab=d2l.load_data_time_machine(batch_size,num_steps)\n",
    "\n",
    "#1.初始化模型参数\n",
    "#num_hiddens为可调的超参数；训练模型时，输入和输出来自相同的词表，具有相同的维度\n",
    "def get_params(vocab_size,num_hiddens,device): #device用于使用GPU\n",
    "    num_inputs=num_outputs=vocab_size\n",
    "    \n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape,device=device)*0.1 #torch.randn返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义\n",
    "    \n",
    "    #隐藏层参数\n",
    "    W_xh=normal((num_inputs,num_hiddens))\n",
    "    W_hh=normal((num_hiddens,num_hiddens))\n",
    "    b_h=torch.zeros(num_hiddens,device=device)\n",
    "    #输出层参数\n",
    "    W_hq=normal((num_hiddens,num_outputs))\n",
    "    b_q=torch.zeros(num_outputs,device=device)\n",
    "    #附加梯度\n",
    "    params=[W_xh,W_hh,b_h,W_hq,b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "    \n",
    "#2.RNN模型\n",
    "#初始化状态函数，返回隐状态\n",
    "def init_rnn_state(batch_size,num_hiddens,device):\n",
    "    return (torch.zeros((batch_size,num_hiddens),device=device),)\n",
    "\n",
    "#RNN运算函数，一个时间步内的隐状态和输出计算\n",
    "def rnn(inputs,state,params): #inputs 形状：(num_steps,batch_size,vocab_size)\n",
    "    W_xh,W_hh,b_h,W_hq,b_q=params\n",
    "    H,=state\n",
    "    outputs=[]\n",
    "    for X in inputs:  #X 形状：(batch_size,vocab_size)\n",
    "        H=torch.tanh(torch.mm(X,W_xh)+torch.mm(H,W_hh)+b_h)\n",
    "        Y=torch.mm(H,W_hq)+b_q\n",
    "        outputs.append(Y) \n",
    "    return outputs,(H,)\n",
    "\n",
    "#3.RNN模型类\n",
    "class RNNModelScratch:\n",
    "    def __init__(self,vocab_size,num_hiddens,device,\n",
    "                get_params,init_state,forward_fn):\n",
    "        self.vocab_size,self.num_hiddens=vocab_size,num_hiddens\n",
    "        self.params=get_params(vocab_size,num_hiddens,device)\n",
    "        self.init_state,self.forward_fn=init_state,forward_fn\n",
    "        \n",
    "    def __call__(self,X,state):\n",
    "        X=F.one_hot(X.T,self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X,state,self.params)\n",
    "    \n",
    "    def begin_state(self,batch_size,device):\n",
    "        return self.init_state(batch_size,self.num_hiddens,device)\n",
    "    \n",
    "#实例化网络\n",
    "num_hiddens=512\n",
    "net=RNNModelScratch(len(vocab),num_hiddens,d2l.try_gpu(),get_params,\n",
    "                   init_rnn_state,rnn)\n",
    "state=net.begin_state(X.shape[0],d2l.try_gpu())\n",
    "Y,new_state=net(X.to(d2l.try_gpu()),state)\n",
    "\n",
    "#定义预测函数，生成prefix之后的新字符\n",
    "def predict_ch8(prefix,num_preds,net,vocab,device):\n",
    "    state=net.begin_state(batch_size=1,device=device)\n",
    "    outputs=[vocab[prefix[0]]]\n",
    "    get_input=lambda:torch.tensor([outputs[-1]].device=device).reshape((1,1))\n",
    "    for y in prefix[1:]:  #预热期\n",
    "        _,state=net(get_input(),state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds): #预测num_preds步\n",
    "        y,state=net(get_input(),state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "        \n",
    "#4.梯度裁剪,避免梯度爆炸\n",
    "def grad_clipping(net,theta):\n",
    "    if isinstance(net,nn.Module):\n",
    "        params=[p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params=net.params\n",
    "    norm=torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:]*=theta/norm\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8.5 RNN模型-使用深度学习框架高级API###\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "    \n",
    "#加载数据\n",
    "batch_size,num_steps=32,35\n",
    "train_iter,vocab=d2l.load_data_time_machine(batch_size,num_steps)\n",
    "\n",
    "#定义模型\n",
    "num_hiddens=256\n",
    "rnn_layer=nn.RNN(len(vocab),num_hiddens) #构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer\n",
    "state=torch.zeros((1,batch_size,num_hiddens)) #使用张量初始化隐状态\n",
    "X=torch.rand(size=(num_steps,batch_size,len(vocab)))\n",
    "Y,state_new=rnn_layer(X,state) #Y是指每个时间步的隐状态，不涉及输出层计算，用作后续输出层的输入\n",
    "\n",
    "#RNN模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,rnn_layer,vocab_size,**kwargs): #构造函数\n",
    "        super(RNNModel,self).__init__(**kwargs)\n",
    "        self.rnn=rnn_layer\n",
    "        self.vocab_size=vocab_size\n",
    "        self.num_hiddens=self.rnn.hidden_size\n",
    "        if not self.rnn.bidirectional: # 如果RNN是双向的（之后将介绍），`num_directions`应该是2，否则应该是1\n",
    "            self.num_directions=1\n",
    "            self.linear=nn.Linear(self.num_hiddens,self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions=2\n",
    "            self.linear=nn.Linear(self.num_hiddens*2,self.vocab_size)\n",
    "       \n",
    "    def forward(self,inputs,state): #前向传播函数\n",
    "        X=F.one_hot(input.T.long(),self.vocab_size) #对输入进行onehot编码\n",
    "        X=X.to(torch.float32)\n",
    "        Y,state=self.rnn(X,state)\n",
    "        output=self.linear(Y.reshape((-1,Y.shape[-1]))) #Y.shape[-1]取Y最后一维的大小\n",
    "        return output,state\n",
    "    \n",
    "    def begin_state(self,device,batch_size=1):\n",
    "        if not isinstance(self.rnn,nn.LSTM): #isinstance(self.rnn,nn.LSTM)当前rnn类型与nn.LSTM相同，返回True，否则返回False\n",
    "            #非LSTM网络，以张量作为隐状态\n",
    "            return torch.zeros((self.num_directions*self.rnn.num_layers,batch_size,self.num_hiddens),device=device)\n",
    "        else:\n",
    "            #LSTM网络，以元组作为隐状态\n",
    "            return (torch.zeros((self.num_directions*self.rnn.num_layers,batch_size,self.num_hiddens),device=device)),\n",
    "                    torch.zeros((self.num_directions*self.rnn.num_layers,batch_size,self.num_hiddens),device=device))\n",
    "\n",
    "#基于具有随机权重的模型进行预测\n",
    "device=d2l.try_gpu()\n",
    "net=RNNModel(rnn_layer,vocab_size=len(vocab))\n",
    "net=net.to(device)\n",
    "d2l.predict_ch8('time traveller',10,net,vocab,device)\n",
    "\n",
    "#使用超参数进行训练\n",
    "num_epochs,lr=500,1\n",
    "d2l.train_ch8(net,train_iter,vocab,lr,num_epochs,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "883a9b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=numpy.array([1,2,3,4])\n",
    "#x.shape[-1]\n",
    "#y=x.reshape((2,2))\n",
    "x.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6ce17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[4, 5, 7, 8]\n",
      "[(1, 4), (2, 5), (3, 7), (4, 8)]\n"
     ]
    }
   ],
   "source": [
    "x=[1,2,3,4,3]\n",
    "y=[3,4,5,7,8]\n",
    "\n",
    "print(x[:-1])\n",
    "print(y[1:])\n",
    "\n",
    "z=zip(x[:-1],y[1:])\n",
    "seq=[pair for pair in z]\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bb90472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 8, 12, 16]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8, 12, 16, 4, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=list(range(0,4*5,4))\n",
    "print(x)\n",
    "\n",
    "random.shuffle(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "249494c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10//4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
