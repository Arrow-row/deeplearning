{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.1 门控循环单元GRU  \n",
    "\n",
    "###(1)从零实现\n",
    "import torch\n",
    "from torch inport nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#读取数据\n",
    "batch_size,num_steps=32,35\n",
    "train_iter,vocab=d2l.load_data_time_machine(batch_size,num_steps)\n",
    "\n",
    "#初始化模型参数\n",
    "def get_params(vocab_size,num_hiddens,device):\n",
    "    num_inputs=num_outputs=vocab_size\n",
    "    \n",
    "    def normal(shape): #权重函数，从标准差为0.01的高斯分布中提前权重张量\n",
    "        return torch.randn(size=shape,device=device)*0.01\n",
    "\n",
    "    def three(): #参数初始化函数\n",
    "        return (normal((num_inputs,num_hiddens)),\n",
    "               normal((num_hiddens,num_hiddens)),\n",
    "               torch.zeros(num_hiddens,device=device))\n",
    "    \n",
    "    W_xz,W_hz,b_z=three() #更新门参数\n",
    "    W_xr,W_hr,b_r=three() #重置门参数\n",
    "    W_xh,W_hh,b_h=three() #候选隐状态参数\n",
    "    W_hq=normal((num_hiddens,num_outputs)),b_q=torch.zeros(num_outputs,device=device) #输出层参数\n",
    "    #附加梯度\n",
    "    params=[W_xz,W_hz,b_z,W_xr,W_hr,b_r,W_xh,W_hh,b_h,W_hq,b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad(True)\n",
    "    return params\n",
    "\n",
    "#定义模型\n",
    "#定义隐状态初始化函数\n",
    "def init_gru_state(batch_size,num_hiddens,device): #隐状态初始化函数，返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零\n",
    "    return (torch.zeros((batch_size,num_hiddens),device=device),)\n",
    "#定义GRU模型\n",
    "def gru(inputs,state,params): #GRU前向传播函数\n",
    "    W_xz,W_hz,b_z,W_xr,W_hr,b_r,W_xh,W_hh,b_h,W_hq,h_q=params\n",
    "    H,=state\n",
    "    outputs=[]\n",
    "    for X in inputs:\n",
    "        Z=torch.sigmoid((X @ W_xz)+(H @ W_hz)+b_z) #更新门输出值\n",
    "        R=torch.sigmoid((X @ W_xr)+(H @ W_hr)+b_r) #重置门输出值\n",
    "        H_tilda=torch.tanh((X @ W_xh)+((R * H) @ W_hh)+b_h) #候选隐状态\n",
    "        H=Z * H+(1-Z)*H_tilda #隐状态\n",
    "        Y=H @ W_hq+b_q #网络输出值\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs,dim=0),(H,)\n",
    "\n",
    "#训练与预测\n",
    "vocab_size,num_hiddens,device=len(vocab),256,d2l.try_gpu()\n",
    "num_epochs,lr=500,1\n",
    "model=d2l.RNNModelScratch(len(vocab),num_hiddens,device,get_params,\n",
    "                         init_gru_state,gru)\n",
    "d2l.train_ch8(model,train_iter,vocab,lr,num_epochs,device)\n",
    "\n",
    "\n",
    "####(2)简洁实现，使用框架中的API\n",
    "num_inputs=vocab_size\n",
    "gru_layer=nn.GRU(num_inputs,num_hiddens)\n",
    "model=d2l.RNNModel(gru_layer,len(vocab))\n",
    "model=model.to(device)\n",
    "d2l.train_ch8(model,train_iter,vocab,lr,num_epochs,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##9.2 长短期记忆网络LSTM \n",
    "\n",
    "#(1)从0开始实现\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#加载数据\n",
    "batch_size,num_steps=32,35\n",
    "train_iter,vocab=d2l.load_data_time_machine(batch_size,num_steps)\n",
    "\n",
    "#初始化模型参数\n",
    "def get_lstm_params(vocab_size,num_hiddens,device): #vocab_size为词表大小，超参数num_hiddens为隐藏单元数量\n",
    "    num_inputs=num_outputs=vocab_size\n",
    "    \n",
    "    def normal(shape): #权重参数初始化函数\n",
    "        return torch.randn(size=shape,device=device)*0.01 #按照标准差0.01的高斯分布初始化权重参数\n",
    "    \n",
    "    def three(): #参数初始化函数，偏置b设置为0\n",
    "        return (normal((num_inputs,num_hiddens)),\n",
    "               normal((num_hiddens,num_hiddens)),\n",
    "               torch.zeros(num_hiddens,device=device))\n",
    "    \n",
    "    W_xi,W_hi,b_i=three() #输入门参数\n",
    "    W_xf,W_hf,b_f=three() #遗忘门参数\n",
    "    W_xo,W_ho,b_o=three() #输出门参数\n",
    "    W_xc,W_hc,b_c=three() #候选门参数\n",
    "    W_hq=normal((num_hiddens,num_outputs)),b_q=torch.zeros(num_outputs,device=device) #输出层参数\n",
    "    #附加梯度\n",
    "    params=[W_xi,W_hi,b_i,W_xf,W_hf,b_f,W_xo,W_ho,b_o,W_xc,W_hc,b_c,W_hq,b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "#定义模型\n",
    "#初始化状态函数\n",
    "def init_lstm_state(batch_size,num_hiddens,device): #状态初始化函数\n",
    "    return (torch.zeros((batch_size,num_hiddens),device=device),\n",
    "            torch.zeros((batch_size,num_hiddens),device=device))\n",
    "\n",
    "#LSTM模型\n",
    "def lstm(inputs,state,params):\n",
    "    [W_xi,W_hi,b_i,W_xf,W_hf,b_f,W_xo,W_ho,b_o,W_xc,W_hc,b_c,W_hq,b_q]=params\n",
    "    (H,C)=state\n",
    "    outputs=[]\n",
    "    for X in inputs:\n",
    "        I=torch.sigmoid((X @ W_xi)+(H @ W_hi)+b_i)\n",
    "        F=torch.sigmoid((X @ W_xf)+(H @ W_hf)+b_f)\n",
    "        O=torch.sigmoid((X @ W_xo)+(H @ W_ho)+b_o)\n",
    "        C_t=torch.tanh((X @ W_xc)+(H @ W_hc)+b_c)\n",
    "        C=F * C+I * C_t\n",
    "        H=O * torch.tanh(C)\n",
    "        Y=(H @ W_hq)+b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs,(H,C)\n",
    "\n",
    "#训练和预测\n",
    "vocab_size,num_hiddens,device=len(vocab),256,d2l.try_gpu()\n",
    "num_epochs,lr=500,1\n",
    "model=d2l.RNNModelScratch(len(vocab),num_hiddens,device,get_lstm_params,init_lstm_state,lstm)\n",
    "d2l.train_ch8(model,train_iter,vocab,lr,num_epochs,device)\n",
    "\n",
    "\n",
    "#(2)简洁实现-使用框架的API\n",
    "num_inputs=vocab_size\n",
    "lstm_layer=nn.LSTM(num_inputs,num_hiddens)\n",
    "model=d2l.RNNModel(lstm_layer,len(vocab))\n",
    "model=model.to(device)\n",
    "d2l.train_ch8(model,train_iter,vocab,lr,num_epochs,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c85dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###9.3 深度循环神经网络 deep_rnn\n",
    "\n",
    "#(1)简洁实现:Deep-LSTM网络-使用框架API\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#加载数据\n",
    "batch_size,num_steps=32,35\n",
    "train_iter,vocab=d2l.load_data_time_machine(batch_size,num_steps)\n",
    "\n",
    "#模型实例化\n",
    "vocab_size,num_hiddens,num_layers=len(vocab),256,2  #num_layers用于设定隐藏层数\n",
    "num_inputs=vocab_size\n",
    "device=d2l.try_gpu()\n",
    "lstm_layer=nn.LSTM(num_inputs,num_hiddens,num_layers)\n",
    "model=d2l.RNNModel(lstm_layer,len(vocab))\n",
    "model=model.to(device)\n",
    "\n",
    "#训练与预测\n",
    "#由于使用了长短期记忆网络模型来实例化两个层，训练速度会大大降低\n",
    "num_epochs,lr=500,2\n",
    "d2l.train_ch8(model,train_iter,vocab,lr,num_epochs,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a13e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###9.4 双向循环神经网络 bi_rnn\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#加载数据\n",
    "batch_size,num_steps,device=32,35,d2l.try_gpu()\n",
    "train_iter,vocab=d2l.load_data_time_machine(batch_size,num_steps)\n",
    "#通过设置\"bidirective=True\"来定义双向LSTM模型\n",
    "vocab_size,num_hiddens,num_layers=len(vocab),256,2\n",
    "num_inputs=vocab_size\n",
    "lstm_layer=nn.LSTM(num_inputs,num_hiddens,num_layers,bidirectional=True)\n",
    "model=d2l.RNNModel(lstm_layer,len(vocab))\n",
    "model=model.to(device)\n",
    "#训练模型\n",
    "num_epochs,lr=500,1\n",
    "d2l.train_ch8(model,train_iter,vocab,lr,num_epochs,device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
