{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. AlexNet\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "net=nn.Sequential(\n",
    "    #输入通道数1，输出通道数96，卷积核窗口大小11，步幅4，填充1\n",
    "    nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    \n",
    "    #减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "    nn.Conv2d(96,256,kernel_size=5,padding=2),nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    \n",
    "    #使用3个连续卷积层和较小的卷积窗口；除了最后的卷积层，输出通道数量进一步增加；最后一个卷积层后使用池化层\n",
    "    nn.Conv2d(256,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,256,kernel_size=3,padding=1),nn.ReLu(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    \n",
    "    #flatten化\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    #2个全连接层，使用dropout层减轻过拟合\n",
    "    nn.Linear(6400,4096),nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096,4096),nn.ReLu(),\n",
    "    nn.Dropout(p=0.5)\n",
    "    \n",
    "    #输出层\n",
    "    nn.Linear(4096,10)\n",
    ")\n",
    "\n",
    "#构造高度、宽度为224的单通道数据，观察net每一层输出形状\n",
    "X=torch.randn(1,1,224,224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n",
    "\n",
    "#读取数据集\n",
    "batch_size=128\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size,resize=224)\n",
    "\n",
    "#训练AlexNet\n",
    "lr,num_epochs=0.01,10\n",
    "d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. VGG\n",
    "'''\n",
    "原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 \n",
    "第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。\n",
    "由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#VGG块\n",
    "def vgg_block(num_convs,in_channels,out_channels):  #输入参数分别为卷积层数量，输入通道数，输出通道数、\n",
    "    layers=[]\n",
    "    for i in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels,out_channels.kernel_size=3,padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels=out_channels #VGG块内部，上一层卷积的输出通道数作为下一层的输入通道数\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "conv_arch=((1,64),(1,128),(2,256),(2,512),(2,512))\n",
    "\n",
    "#VGG-11网络\n",
    "def vgg(conv_arch): #此函数实现vgg-11网络\n",
    "    conv_blks=[]\n",
    "    in_channels=1\n",
    "    for (num_convs,out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs,in_channels,out_channels))\n",
    "        in_channels=out_channels #VGG块之间，上一块最后一层的输出通道数作为下一块第一层的输入通道数\n",
    "        \n",
    "    return nn.Sequential(*conv_blks,nn.Flatten(),\n",
    "                        #全连接层部分\n",
    "                        nn.Linear(out_channels*7*7,4096),nn.ReLU,nn.Dropout(0.5),\n",
    "                        nn.Linear(4096,4096),nn.ReLU(),nn.Dropout(0.5),\n",
    "                        nn.Linear(4096,10))\n",
    "\n",
    "net=vgg(conv_arch)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
