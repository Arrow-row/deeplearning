{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. AlexNet\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "net=nn.Sequential(\n",
    "    #输入通道数1，输出通道数96，卷积核窗口大小11，步幅4，填充1\n",
    "    nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    \n",
    "    #减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "    nn.Conv2d(96,256,kernel_size=5,padding=2),nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    \n",
    "    #使用3个连续卷积层和较小的卷积窗口；除了最后的卷积层，输出通道数量进一步增加；最后一个卷积层后使用池化层\n",
    "    nn.Conv2d(256,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,256,kernel_size=3,padding=1),nn.ReLu(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    \n",
    "    #flatten化\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    #2个全连接层，使用dropout层减轻过拟合\n",
    "    nn.Linear(6400,4096),nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096,4096),nn.ReLu(),\n",
    "    nn.Dropout(p=0.5)\n",
    "    \n",
    "    #输出层\n",
    "    nn.Linear(4096,10)\n",
    ")\n",
    "\n",
    "#构造高度、宽度为224的单通道数据，观察net每一层输出形状\n",
    "X=torch.randn(1,1,224,224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n",
    "\n",
    "#读取数据集\n",
    "batch_size=128\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size,resize=224)\n",
    "\n",
    "#训练AlexNet\n",
    "lr,num_epochs=0.01,10\n",
    "d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. VGG\n",
    "'''\n",
    "原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 \n",
    "第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。\n",
    "由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#VGG块\n",
    "def vgg_block(num_convs,in_channels,out_channels):  #输入参数分别为卷积层数量，输入通道数，输出通道数、\n",
    "    layers=[]\n",
    "    for i in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels,out_channels.kernel_size=3,padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels=out_channels #VGG块内部，上一层卷积的输出通道数作为下一层的输入通道数\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "conv_arch=((1,64),(1,128),(2,256),(2,512),(2,512))\n",
    "\n",
    "#VGG-11网络\n",
    "def vgg(conv_arch): #此函数实现vgg-11网络\n",
    "    conv_blks=[]\n",
    "    in_channels=1\n",
    "    for (num_convs,out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs,in_channels,out_channels))\n",
    "        in_channels=out_channels #VGG块之间，上一块最后一层的输出通道数作为下一块第一层的输入通道数\n",
    "        \n",
    "    return nn.Sequential(*conv_blks,nn.Flatten(),\n",
    "                        #全连接层部分\n",
    "                        nn.Linear(out_channels*7*7,4096),nn.ReLU,nn.Dropout(0.5),\n",
    "                        nn.Linear(4096,4096),nn.ReLU(),nn.Dropout(0.5),\n",
    "                        nn.Linear(4096,10))\n",
    "\n",
    "net=vgg(conv_arch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed93566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NiN\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# NiN块\n",
    "def nin_block(in_channels,out_channels,kernel_size,strides,padding):\n",
    "    return nn.Sequential(\n",
    "        #1个普通卷积层\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding),nn.ReLU(),\n",
    "        #2个1x1卷积层\n",
    "        nn.Conv2d(out_channels,out_channels,kernel_size=1),nn.ReLU(),\n",
    "        nn.Conv2d(out_channels,out_channels,kernel_size=1),nn.ReLU())\n",
    "\n",
    "# NiN网络\n",
    "net=nn.Sequential(\n",
    "    nin_block(1,96,kernel_size=11,strides=4,padding=0),\n",
    "    nn.MaxPool2d(3,stride=2),\n",
    "    nin_block(96,256,kernel_size=5,strides=1,padding=2),\n",
    "    nn.MaxPool2d(3,strides=2),\n",
    "    nin_block(256,384,kernel_size=3,strides=1,padding=1),\n",
    "    nn.MaxPool2d(3,strides=2),\n",
    "    nn.Dropout(0.5),\n",
    "    #类别标签数为10\n",
    "    nin_block(384,10,kernel_size=3,strides=1,padding=1),\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    #flatten化，将四维输出转换成二维输出，二维的形状为 (batch_size,10)\n",
    "    nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e73930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. GoogLeNet\n",
    "import torch\n",
    "from  torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#Inception块\n",
    "class Iception(nn.Module):\n",
    "    #构造函数，`c1`--`c4` 是每条路径的输出通道数\n",
    "    def __init__(self,in_chanels,c1,c2,c3,c4,**kwargs):\n",
    "        super(Inception,self).__init__(**kwargs)\n",
    "        #路径1，单1x1卷积层\n",
    "        self.p1_1=nn.Conv2d(in_chanels,c1,kernel_size=1)\n",
    "        #路径2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1=nn.Conv2d(in_chanels,c2[0],kernel_size=1)\n",
    "        self.p2_2=nn.Conv2d(c2[0],c2[1],kernel_size=3,padding=1)\n",
    "        #路径3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1=nn.Conv2d(in_chanels,c3[0],kernel_size=1)\n",
    "        self.p3_2=nn.Conv2d(c3[0],c3[1],kernel_size=5,padding=2)\n",
    "        #路径4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1=nn.MaxPool2d(kernel_size=3,stride=1,padding=1)\n",
    "        self.p4_2=nn.Conv2d(in_chanels,c4,kernel_size=1)\n",
    "    \n",
    "    #模块的前向传播函数\n",
    "    def forward(self,x): \n",
    "        p1=F.relu(self.p1_1(x))\n",
    "        p2=F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3=F.relu(self.p3_2(F.relu(slef.p3_1(x))))\n",
    "        p4=F.relu(self.p4_2(self.p4_1(x)))\n",
    "        #在通道维度上连结输出\n",
    "        return torch.cat((p1,p2,p3,p4),dim=1)\n",
    "\n",
    "#GoogLeNet网络\n",
    "b1=nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3,stride=2,padding=1))\n",
    "b2=nn.Sequential(Conv2d(64,64,kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64,192,kernel_size=3,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3,stride=2,padding=1))\n",
    "b3=nn.Sequential(Inception(192,64,(96,128),(16,32),32),\n",
    "                Inception(256,128,(128,192),(32,96),64),\n",
    "                nn.MaxPool2d(kernel_size=3,stride=2,padding=1))\n",
    "b4=nn.Sequential(Inception(480,192,(96,208),(16,48),64),\n",
    "                Inception(512,160,(112,224),(24,64),64),\n",
    "                Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "b5=nn.Sequential(Inception(832,256,(160,320),(32,128),128),\n",
    "                Inception(832,384,(192,384),(48,128),128),\n",
    "                nn.AdaptiveAvgPool2d(1,1),\n",
    "                nn.Flatten())\n",
    "\n",
    "net=nn.Sequential(b1,b2,b3,b4,b5,nn.Linear(1024,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. batch-norm 批量规范化\n",
    "\n",
    "# 5.1 从零实现批量规范化层\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#批量规范化算法\n",
    "def batch_norm(X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    if not torch.is_grad_enabled(): #通过 is_grad_enabled 判断当前模式是训练模式还是预测模式\n",
    "        X_hat=(X-moving_mean)/torch.sqrt(moving_var+eps) #预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4) #通过输入变量X的形状判断当前是作用于全连接层还是卷积层\n",
    "        if len(X.shape)==2:  #当前输入X作用于全连接层，计算特征维上的均值和方差\n",
    "            mean=X.mean(dim=0)  \n",
    "            var=((X=mean)**2).mean(dim=0)\n",
    "        else: #当前输入X作用于二维卷积层，计算通道维(axis=1)的均值和方差；这里需要保持X的形状以便后面可以做广播运算\n",
    "            mean=X.mean(dim=(0,2,3),keepdim=True)\n",
    "            var=((X-mean)**2).mean(dim=(0,2,3),keepdim=True)\n",
    "        X_hat=(X-mean)/torch.sqrt(var+eps) #训练模式下，用当前均值和方差做标准化\n",
    "        #更新移动平均的均值和方差\n",
    "        moving_mean=momentum*moving_mean+(1.0-momentum)*mean\n",
    "        moving_var=momentum*moving_var+(1.0-momentum)*var\n",
    "    Y=gamma*X_hat+beta #缩放和移位\n",
    "    return Y,moving_mean.data,moving_var.data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
