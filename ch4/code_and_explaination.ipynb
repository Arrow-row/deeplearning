{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723eb13c",
   "metadata": {},
   "source": [
    "## 1.多层感知机从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1175b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp-scratch ------多层感知机从零实现\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#数据导入\n",
    "batch_size=256\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "#初始化模型参数\n",
    "num_inputs,num_outputs,num_hiddens=784,10,256 #输入，输出，隐藏层隐藏单元\n",
    "W1=nn.Parameter(torch.randn(num_inputs,num_hiddens,requires_grad=True)*0.01) #输入层到隐藏层\n",
    "b1=nn.Parameter(torch.zeros(num_hiddens,require_grad=True))\n",
    "W2=nn.Parameter(torch.randn(num_inputs,num_hiddens,require_grad=True)*0.01) #隐藏层到输出层\n",
    "b2=nn.Parameter(torch.zeros(num_outputs,require_grad=True))\n",
    "\n",
    "params=[W1,b1,W2,b2]\n",
    "\n",
    "#激活函数：实现ReLU函数\n",
    "def relu(X):\n",
    "    a=torch.zeros_like(X)\n",
    "    return torch.max(X,a)\n",
    "\n",
    "#定义模型\n",
    "def net(X):\n",
    "    X=X.reshape((-1,num_inputs)) #将每个二维图像转换为一个长度为num_inputs的向量\n",
    "    H=relu(X@W1+b1) #这里@表示矩阵乘法\n",
    "    return (H@W2+b2)\n",
    "\n",
    "#损失函数\n",
    "loss=nn.CrossEntropyLoss() #使用内置API计算交叉熵损失\n",
    "\n",
    "#训练：与softmax训练过程完全相同\n",
    "num_epochs,lr=10,0.1\n",
    "updater=torch.optim.SGD(params,lr=lr)\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,updater)\n",
    "\n",
    "#模型应用于测试数据\n",
    "d2l.predict_ch3(net,test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75baa4cc",
   "metadata": {},
   "source": [
    "## 2.多层感知机简洁实现--使用API构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp-concise ------多层感知机简洁实现--使用API构建网络\n",
    "import torch\n",
    "feom torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#模型\n",
    "net=nn.Sequential(nn.Flatten(),\n",
    "                 nn.Linear(784,256),\n",
    "                 nn.ReLU().\n",
    "                 nn.Linear(256,10))\n",
    "def init_weights(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)\n",
    "net.apply(init_weights);\n",
    "\n",
    "#交叉熵损失函数\n",
    "loss=nn.CrossEntropyLoss() \n",
    "\n",
    "#训练\n",
    "batch_size,lr,num_epochs=256,0.1,10\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size) #数据导入\n",
    "trainer=torch.optim.SGD(net.parameters(),lr=lr) #参数优化\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dedf12",
   "metadata": {},
   "source": [
    "## 3.多项式拟合--模型复杂性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#生成数据\n",
    "max_degree=20 #多项式最大阶数\n",
    "n_train,n_test=100,100 #训练集和测试集大小\n",
    "true_w=np.zeros(max_degree) #分配空间\n",
    "true_w[0:4]=np.array([5,1.2,-3.4,5.6])\n",
    "\n",
    "features=np.random.normal(size=(n_train+ntest,1)) #训练集和测试集均为100x1的正态分布随机数\n",
    "np.random.shuffle(features)\n",
    "poly_features=np.power(features,np.arange(max_degree).reshape(1,-1)) #feature中的数，通过广播机制求每个数的0~19次方，ploy_features.shape=200x20。np.arange(max_degree).reshape(1, -1)将轴数从1调整为2\n",
    "for i in range(max_degree):\n",
    "    poly_features[:,i]/=math.gamma(i,1) #自变量的n次方除以对应阶数n的gamma(n)=(n-1)!\n",
    "labels=np.dot(poly_features,true_w) #各阶数的自变量乘以系数得到输出y。labels维度：(n_train+n_test,)\n",
    "labels+=np.random.normal(scale=0.1,size=labels.shape) #加上噪声\n",
    "true_w,features,poly_features,labels=[torch.tensor(x,dtype=torch.float32) \n",
    "                                     for x in [true_w,features,poly_features,labels]] #numpy ndarray转换为tensor\n",
    "\n",
    "#损失评估----评估给定数据集上模型的\n",
    "def evaluate_loss(net,data_iter,loss): #@save  #输入网络类型，数据集，损失函数\n",
    "    metric=d2l.Accumulator(2) #构建含2个0元素的列表，第1个元素表示损失总和，第2个元素表示元素数目。 实用程序类Accumulator，用于对多个变量进行累加\n",
    "    for X,y in data_iter:\n",
    "        out=net(X) #训练得到预测输出\n",
    "        y=y.reshape(out.shape)\n",
    "        l=loss(out,y) #损失函数计算损失\n",
    "        metrix.add(l.sum(),l.numel()) #numel()返回数组中元素的个数\n",
    "    return metric[0]/metric[1] #返回训练样本的损失均值\n",
    "\n",
    "#训练函数\n",
    "def train(train_features,test_features,train_labels,test_labels,\n",
    "         num_epochs=400):\n",
    "    \n",
    "    #损失函数\n",
    "    loss=nn.MSELoss() #使用均方差损失函数MSELoss\n",
    "    \n",
    "    #网络模型\n",
    "    input_shape=train_features.shape(-1) #shape[-1]获取shape中的最后一个元素，即样本数量\n",
    "    net=nn.Sequential(nn.Linear(input_shape,1,bias=False)) ## 不设置偏置，因为已经在多项式特征中实现了偏置\n",
    "    \n",
    "    #构建训练/测试数据集\n",
    "    batch_size=min(10,train_labels.shape[0]) #获取批量大小\n",
    "    train_iter=d2l.load_array((train_features,train_labels.shape(-1,1),batch_size)) #加载训练数据集\n",
    "    test_iter=d2l.load_array((test_features,test_labels.reshape(-1,1),batch_size,is_train=False))#加载测试数据集\n",
    "    \n",
    "    #优化函数\n",
    "    trainer=torch.opti.SGD(net.parameters(),lr=0.01) #使用随机梯度下降算法更新参数\n",
    "    \n",
    "    #绘制画布、坐标轴、图例\n",
    "    animator=d2l.Animator(xlabel='epoch',ylabel='loss',yscale='log',xlim=[1,num_epochs],ylim=[le-3,le2],legend=['train','test'])#调用d2l.Animator(),绘制训练过程中训练集和测试集上的epoch-loss图\n",
    "    \n",
    "    #训练\n",
    "    for epoch in range(num_epochs):\n",
    "        d2l.train_epoch_ch3(net,train_iter,loss,trainer) #调用训练函数进行训练，不断更新参数--多项式系数\n",
    "        if epoch==0 or (epoch+1)%20==0: #由训练进度绘制数据点，分别绘制训练集和测试集平均损失。evaluate_loss()计算数据集上的损失均值\n",
    "            animator.add(epoch+1,(evaluate_loss(net,train_iter,loss),evaluate_loss(net,test_iter,loss)))\n",
    "    print('weight:',net[0].weight.data.numpy()) #打印多项式系数\n",
    "    \n",
    "#三阶多项式函数拟合\n",
    "train(poly_features[:n_train,:4],poly_features[n_train:,:4],labels[:n_train],labels[n_train:]) #poly_features[:ntrain,:4] 获取前ntrain个样本的前4个特征维度，最高为3阶，即 1, x, x^2/2!, x^3/3!。已定义n_train=100，前100个样本作为训练集，剩余100个样本最为测试集\n",
    "    \n",
    "#线性函数拟合(欠拟合)\n",
    "train(poly_features[:n_train,:2],poly_features[n_train:,:2],labels[:n_train],labels[n_train:]) #只选择前两个特征，最高阶为1，即1, x\n",
    "\n",
    "#高阶多项式拟合(过拟合)\n",
    "train(poly_features[:n_train,:],poly_features[n_train:,:],labels[:n_train],labels[n_train:]) #选择所有特征，即包含最高阶数19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce325660",
   "metadata": {},
   "source": [
    "## 4.权重衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd84c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
